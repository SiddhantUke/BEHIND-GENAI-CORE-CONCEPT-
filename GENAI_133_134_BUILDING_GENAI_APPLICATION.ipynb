{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNykzPYLuSf9F1R87czhm8y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiddhantUke/BEHIND-GENAI-CORE-CONCEPT-/blob/main/GENAI_133_134_BUILDING_GENAI_APPLICATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## What type of Application we gonna generate over here !\n",
        "\n",
        "'''\n",
        "we have some website and have some kind or text/content format !\n",
        "\n",
        "so we need to extract that from\n",
        "in langchain we have diff. data ingestion technique that is web based loader !\n",
        "\n",
        "we scarp the entire content ! from the website !!\n",
        "\n",
        "after that we gonna divide and then convert intp chunk and then try to do vector\n",
        "\n",
        "embedding technique ! then we use llm along with prompt engineering !\n",
        "\n",
        "to Specifically get the output from that particular page !"
      ],
      "metadata": {
        "id": "M9eNdtDgKcMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "what we are planning ?\n",
        "\n",
        "LEts take the website and then copy the link of that website and then after that we have to scrap the website with the help of llm"
      ],
      "metadata": {
        "id": "l4BLzCgrRlOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Simple Gen AI Using LAngchain with the help of OPENAI\n",
        "\n",
        " ## Load all the keys\n",
        "## fOR RETRIVED THE ENTIRE CONTENT OF THE IMFORMATION I HAVE TO READ THE ENTIRE CONTENT\n",
        "\n",
        "## tHEN\n",
        "\n",
        "\"\"\" import the library that is \"\"\"   ## For scraping rhe data !\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n-Ir3HgDwnzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On8egHB1C52J",
        "outputId": "acc77fe7-be72-4207-d06a-0e504d39fb6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os ## Library in python for setup our environment variable !\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv() ## Specifically use to import all the important that is avialble in the --------\n",
        "\n",
        "## Setup the OPENAI API KEYS\n",
        "\n",
        "\n",
        "os.environ['OPEN_API_KEY'] = os.getenv(\"OPEN_API_KEY\")\n",
        "\n",
        "## For langsmith tracking langchain api key !\n",
        "\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LAGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
      ],
      "metadata": {
        "id": "8rSlSAfGke1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6gYD1HIkext",
        "outputId": "57cca187-c8eb-408f-dda0-6db8f4053c53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.66)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.26 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DATA INGESTION -- From the website we need to scrape the data !\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from langchain_community   # =====> ## has lot of community !\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n"
      ],
      "metadata": {
        "id": "NWj4YrXqkevq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader('') ## Inside write link !\n",
        "\n",
        "# after  load the data .\n",
        "\n",
        "#Webbaselloader  -----> whatever the link you have"
      ],
      "metadata": {
        "id": "RcFZw_LmmBfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load&\n",
        "\n",
        "docs #------------------ here is the data we got"
      ],
      "metadata": {
        "id": "sO7_jywuketo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we specifically perfome the last !\n"
      ],
      "metadata": {
        "id": "HTaVR-bYoqNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## In GENAI EVERY SUNGLW A"
      ],
      "metadata": {
        "id": "JSpIkw-koQoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HW962lhQkequ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## load the data ---> Docs ---> Divide our text into chunks ---->text to Vectors --> Vector Embedding  ----> Store in Vector Store Db"
      ],
      "metadata": {
        "id": "8CrcNjqtkejJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "documents=text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "4Zz35A9qked1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "eZGKZ2aPkebH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Converting Text into Vector !\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()\n"
      ],
      "metadata": {
        "id": "Ye_OZ8gMkeYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Here we use Faiss Vector Data Bases @\n",
        "\n",
        "from langchain_community.vectorstores import Faiss\n",
        "## aLL VECTOR STORE IN FAISS\n",
        "\n"
      ],
      "metadata": {
        "id": "lnAkoeAqkeWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "vectorstoredb=FAISS.from_documents(documents,embeddings)"
      ],
      "metadata": {
        "id": "2WdvE4GPkeU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstoredb"
      ],
      "metadata": {
        "id": "_apg1pdckeSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### hERE WE STARTED OUR 134 LECTURE UNDERSTANDING RETRIEVERS AND CHAINS !\n",
        "\n"
      ],
      "metadata": {
        "id": "uqsk4JbFKAbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## bASED ON qUERY WE ASK ANYTHING !\n",
        "\n",
        "## iF I AM USERS AND WANTED SOME RESPONSE ! FROM THAT PARTICULAR WEBSITE OR WE CAN SAY THAT !\n",
        "\n",
        "query = \"write some query here about that same page/web\"\n",
        "\n",
        "\n",
        "result = vectorstoredb.similarity_search(query)\n",
        "\n",
        "result[0].page_content\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "as an o/p we get an answer what we actucally gives with similarity search!\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "-5Dd5kBFkeQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "olTuprw2keOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retriever Chains\n",
        "\n",
        "Suppose if i want to ask a question which is so meaningfull and provide content with respect to that particular picture !\n",
        "\n",
        "thats why we using retriver chains !\n",
        "\n",
        "\n",
        "\n",
        "## In most the rag application we must use this !\n",
        "\n"
      ],
      "metadata": {
        "id": "QTyBxlytLxp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_document import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "'''\n",
        "llm  model has content of any document and to answer much more properly\n",
        "\n",
        "'''\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    '''\n",
        "  Answer the following based only on the provided context:\n",
        "\n",
        "\n",
        "  <context>\n",
        "  {context}\n",
        "  </context>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  '''\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "## THis is the information regarding the documentation and text ! we are giving to llm models !\n",
        "\n",
        "\n",
        "## Create a Document chain !!\n",
        "\n",
        "document_chain = create create_stuff_documents_chain(llm,prompt)\n",
        "\n",
        "## THen given the llm at upper cell !\n",
        "\n",
        "document_chain\n",
        "\n",
        "## For showing the result !\n",
        "\n",
        "# output\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "runnable binding\n",
        "\n",
        "ChatPromptTemplate\n",
        "ChatOpenAI\n",
        "StroutputParswe\n",
        "\n",
        "from going to all this chain it should give me some contxt information !  !!\n",
        "\n",
        "this all are chaom\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "bbqaiz-FkeMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### As we go ahead and use this document chain !\n",
        "\n",
        "document_chain.invoke({\n",
        "   'input':\"put the prompt that actucally we entred !\"\n",
        "   \"context\": [Document (page_content=\"we add same input line as well as add something about the content ! \")\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8Lo7dqNkeKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding the comment for the document !\n",
        "\n",
        "\n",
        "However,we want the documents to first come from the retriver we just setup.\n",
        "That way,we can use the retriever to dynamically select the most reevelent document and pass those in for a given !\n"
      ],
      "metadata": {
        "id": "FFNEuL1wXUp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" What excatly the Retriever \"\"\""
      ],
      "metadata": {
        "id": "Nit_P3kLkeHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Suppose we have vectorr dbb !\n",
        "\n",
        "## Input  ----> Retriver --- > vectorstoredb\n",
        "\n",
        "## Retreiver is the path way that actucally get the data from the"
      ],
      "metadata": {
        "id": "3fK7FkC-keFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstoredb.as_retriever()\n",
        "\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(retriever,document_chain)\n",
        "\n",
        "## Document  Chain responsible for giving you the context information !\n",
        "## Document  chain combined with the retriever chain !\n",
        "\n",
        "\n",
        "retrieval_chain"
      ],
      "metadata": {
        "id": "Mgu7-XNRag2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Get the response from the llm !\n",
        "\n",
        "response  = retrieval_chain.invoke({'input':yaha apni query daalo })\n",
        "\n",
        "response['answer']"
      ],
      "metadata": {
        "id": "NgMkZTqxagkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "X--oUhMHkeCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_bA3Swkpkd_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFfOZdU2kd99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cOxKc-w0kd77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Odht48bkd4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}