{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ4m2UMEdLacEcuX0FcrmZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiddhantUke/BEHIND-GENAI-CORE-CONCEPT-/blob/main/GENAI_132_BUILDING_IMP_OF_LANGCHAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Getting started With Langchain And Open AI\n",
        "\n",
        "In this quickstart we'll see how to:\n",
        "\n",
        "- Get setup with LangChain, LangSmith and LangServe\n",
        "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
        "- Build a simple application with LangChain\n",
        "- Trace your application with LangSmith\n",
        "- Serve your application with LangServe"
      ],
      "metadata": {
        "id": "l4BLzCgrRlOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##lETS EXPORT ALL THE ENVIROMENT VARIABLE WE HAVE !\n",
        " ## ENV VARIBLE THAT IS\n",
        "\n",
        "'''\n",
        "\n",
        " venv\n",
        " .env\n",
        " requirements.txt\n",
        "\n",
        " 1)langchain folder\n",
        "    1.1)openai\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "oZXnX3TzC6OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in requirmeents.txt\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ipykernel\n",
        "langchain\n",
        "python-dotenv\n",
        "Langchain-openai ## FOR ALL THE INBUILT FUNTN REQUIRED WE REALLY NEED TO USE THIS PARTICULAR  LIBRARY !\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S7etEOg5kczh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os ## Library in python for setup our environment variable !\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv() ## Specifically use to import all the important that is avialble in the --------\n",
        "\n",
        "## Setup the OPENAI API KEYS\n",
        "\n",
        "\n",
        "os.environ['OPEN_API_KEY'] = os.getenv(\"OPEN_API_KEY\")\n",
        "\n",
        "## For langsmith tracking langchain api key !\n",
        "\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LAGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n"
      ],
      "metadata": {
        "id": "h9J42P5EC6K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SINCE i want to use langchain with openai we actually wanted to use this  library!\n",
        "\n",
        "from langchain_openai import ChatOpenAI  ## WHY COZ WE WANT TO CREATE A CHATBOT !\n",
        "llm=ChatOpenAI()   ## here we need to put the api key\n",
        "print(llm)\n",
        "\n",
        "\n",
        "'''\n",
        "llm=ChatOpenAI(api_key=\"\")\n",
        "'''"
      ],
      "metadata": {
        "id": "g7i2ZIe8C6Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Whemever we work with llm models !\n",
        "\n",
        "##  What kind of models are there !\n",
        "\n",
        "## Openai gpt4-0\n",
        "\n",
        "## How  to call it !\n",
        "\n",
        "\n",
        "'''\n",
        "llm=ChatOpenAI(model='gpt-4o')\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "OKlOYbwwC6Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  Suppose if wwe want some question then provide some input we want some response !\n",
        "## Input and Get Response From LLM\n",
        "\n",
        "llm.invoke(\"What is GenAi\")\n",
        "\n",
        "\n",
        "## Stored in result variable\n",
        "# result=llm.invoke(\"What is GenAi\")"
      ],
      "metadata": {
        "id": "LoVYvNYmC6Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "id": "1DukpVOOsF0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ## Chatprompt Templates\n",
        "\n",
        "'''\n",
        "Basically we command the llm to act like this and this !\n",
        "\n",
        "'''\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt=ChatPromptTemplate.from_message(\n",
        "    [\n",
        "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the question\")\n",
        "        (\"user\",\"{input}\")             ## Provide by the user based on the input\n",
        "    ]\n",
        ")\n",
        "prompt\n"
      ],
      "metadata": {
        "id": "8H3neFYUC6A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Use this prompt templates along with llm models !\n",
        "\n",
        "chain=prompt|llm  # Combine with multiple things !\n",
        "\n",
        "response = chain.invoke({\"input\":\"Can you tell me about the langsmith\"})\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "QsAvlNiEC5-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Ai nmessgae\n",
        "\n",
        "type(reponse)"
      ],
      "metadata": {
        "id": "iAmKVpoWC57X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The Default o/p Parsor that will be using\n",
        "\n",
        "## Stroutput Parsor\n",
        "\n",
        "\"\"\"\n",
        "basically it is about getting the output from and how you display it\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()\n",
        "chain=prompt|llm|output_parser\n",
        "\n",
        "response = chain.invoke({\"input\":\"Can you tell me about the langsmith\"})\n",
        "print(response)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "UcAY3g8vC55L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0U-No-EwoM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-Ir3HgDwnzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "On8egHB1C52J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}